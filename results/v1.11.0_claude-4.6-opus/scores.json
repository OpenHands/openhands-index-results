[
  {
    "benchmark": "swe-bench",
    "score": 74.8,
    "metric": "accuracy",
    "cost_per_instance": 0.56,
    "average_runtime": 178.0,
    "full_archive": "https://results.eval.all-hands.dev/swebench/litellm_proxy-anthropic-claude-opus-4-6/21809491582/results.tar.gz",
    "tags": [
      "swe-bench"
    ],
    "submission_time": "2026-02-09T07:37:23+00:00",
    "eval_visualization_page": "https://laminar.sh/shared/evals/f2d3e96a-5265-44f5-88b2-434d0a81b893"
  }
]
