[
  {
    "benchmark": "swe-bench-multimodal",
    "score": 26.5,
    "metric": "accuracy",
    "cost_per_instance": 2.55,
    "average_runtime": 694.0,
    "full_archive": "https://storage.googleapis.com/openhands-evaluation-results/eval-21039823837-claude-4-5_litellm_proxy-anthropic-claude-opus-4-5-20251101_26-01-15-21-09.tar.gz",
    "tags": [
      "swe-bench-multimodal"
    ]
  },
  {
    "benchmark": "commit0",
    "score": 50.0,
    "metric": "accuracy",
    "average_runtime": 826,
    "full_archive": "https://storage.googleapis.com/openhands-evaluation-results/eval-20798907628-claude-4-5_litellm_proxy-anthropic-claude-opus-4-5-20251101_26-01-07-23-57.tar.gz",
    "tags": [
      "commit0"
    ],
    "cost_per_instance": 1.077
  },
  {
    "benchmark": "gaia",
    "score": 69.1,
    "metric": "accuracy",
    "average_runtime": 102,
    "full_archive": "https://storage.googleapis.com/openhands-evaluation-results/eval-20805719842-claude-4-5_litellm_proxy-anthropic-claude-opus-4-5-20251101_26-01-08-06-24.tar.gz",
    "tags": [
      "gaia"
    ],
    "cost_per_instance": 0.5473
  },
  {
    "benchmark": "swe-bench",
    "score": 77.6,
    "metric": "accuracy",
    "average_runtime": 0,
    "tags": [
      "swe-bench"
    ],
    "cost_per_instance": 1.9591
  }
]
