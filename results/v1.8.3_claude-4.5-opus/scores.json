[
  {
    "benchmark": "commit0",
    "score": 37.5,
    "metric": "accuracy",
    "cost_per_instance": 4.65,
    "average_runtime": 495.0,
    "full_archive": "https://results.eval.all-hands.dev/eval-21324954369-claude-4-5_litellm_proxy-anthropic-claude-opus-4-5-20251101_26-01-25-03-56.tar.gz",
    "tags": [
      "commit0"
    ]
  },
  {
    "benchmark": "gaia",
    "score": 69.1,
    "metric": "accuracy",
    "cost_per_instance": 0.55,
    "average_runtime": 97.0,
    "full_archive": "https://results.eval.all-hands.dev/eval-20805719842-claude-4-5_litellm_proxy-anthropic-claude-opus-4-5-20251101_26-01-08-06-24.tar.gz",
    "tags": [
      "gaia"
    ]
  },
  {
    "benchmark": "swe-bench",
    "score": 77.6,
    "metric": "accuracy",
    "average_runtime": 0,
    "tags": [
      "swe-bench"
    ],
    "cost_per_instance": 1.9591
  },
  {
    "benchmark": "swt-bench",
    "score": 78.5,
    "metric": "accuracy",
    "cost_per_instance": 1.38,
    "average_runtime": 268.0,
    "full_archive": "https://results.eval.all-hands.dev/eval-21173239168-claude-4-5_litellm_proxy-anthropic-claude-opus-4-5-20251101_26-01-20-19-27.tar.gz",
    "tags": [
      "swt-bench"
    ]
  },
  {
    "benchmark": "swe-bench-multimodal",
    "score": 41.2,
    "metric": "solveable_accuracy",
    "cost_per_instance": 2.54,
    "average_runtime": 671.0,
    "full_archive": "https://results.eval.all-hands.dev/eval-21323385943-claude-4-5_litellm_proxy-anthropic-claude-opus-4-5-20251101_26-01-25-04-21.tar.gz",
    "tags": [
      "swe-bench-multimodal"
    ],
    "component_scores": {
      "solveable_accuracy": 41.2,
      "unsolveable_accuracy": 0.0,
      "combined_accuracy": 27.5,
      "solveable_resolved": 28,
      "solveable_total": 68,
      "unsolveable_resolved": 0,
      "unsolveable_total": 34
    }
  }
]
